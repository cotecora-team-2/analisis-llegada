---
title: "Simulación de tiempos de llegada"
output: html_document
---

```{r setup, include=FALSE, message = FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
theme_set(theme_minimal(base_size = 13))
knitr::opts_chunk$set(echo = TRUE)
# paquetes y codigo 
library(lubridate)
library(patchwork)
library(survival)
library(survminer)
library(ggfortify)
estados <- c("ZAC", "COL", "CHIH", "MICH", "NAY")
source("./simulacion_tiempos.R")
source("./lectura_procesamiento.R")
```


## Datos

Utilizaremos los datos de la elección presidencial de 2018.

- Muestra seleccionada para el conteo rápido.
- Remesas de la muestra seleccionada, hasta media noche.
- Tabla de conteos distritales por casilla

## Idea para simulación

1. Construir para cada estado un modelo para los tiempos de llegada de las casillas
según los datos de remesas, en términos de lista nominal, tipo de casilla, porcentaje
de votos obtenidos para los candidatos y otras variables.
2. Seleccionar una muestra nueva a partir de la tabla de conteos distritales y según
el diseño.
3. Simular tiempos de llegada según 1)
4. Evaluar propiedades de estimadores (por ejempĺo cobertura para intervalos, o sesgo
para estimadores puntuales) dependiendo de la hora de corte, o
cortes basados en % de casillas recibidas en las remesas simuladas. 

En esta versión, consideramos la estimación de proporción de votos para un candidato,
y por el momento usamos muestreo aleatorio simple para ilustrar.

## Modelos de tiempo de llegada

- Usamos modelos paramétricos de supervivencia (*survreg*) para los tiempos
de llegada de las casillas, pues una fracción de la muestra 
- Estos modelos son estratificados por estado, y en este ejemplo usamos tiempos
de llegada log-normales.
- Algunos diagnósticos del ajuste están en la siguiente sección

```{r}
estados 
```


```{r, echo = FALSE, message = FALSE}
library(broom)
media_ln_log  <- mean(llegadas_tbl$lista_nominal_log) 
llegadas_tbl_2 <- llegadas_tbl %>% filter(state_abbr %in% estados) %>%
                  ungroup %>% 
                  mutate(grupo_ln = cut_number(LISTA_NOMINAL, 3)) %>% 
                  mutate(tiempo_huso = ifelse(tiempo - huso > 0, tiempo - huso, 0.001)) %>% 
                  mutate(ln_log_c = lista_nominal_log - mean(lista_nominal_log)) 
reg_2 <- survreg(Surv(tiempo_huso, status) ~ 1 +
                   ln_log_c +
                   tipo_casilla +
                   #tipo_seccion +
                   #state_abbr:log(1+RAC_1/total_3) + 
                   state_abbr:log(1+AMLO_1/(TOTAL_VOTOS_CALCULADOS + 1)) + 
                   state_abbr:log(1+JAMK_1/(TOTAL_VOTOS_CALCULADOS +  1)) +
                   state_abbr:ln_log_c + 
                   #state_abbr:tipo_seccion:ln_log_c + 
                   strata(state_abbr), 
                   llegadas_tbl_2, 
                 dist='lognormal', control = survreg.control(maxiter = 5000))
```

Los coeficientes y escalas para cada estado son:

```{r}
reg_2 %>% tidy() %>%  DT::datatable()
reg_2$scale
```


### Diagnósticos

Checamos simulaciones del modelo con la misma muestra para ver el ajuste a total.
En  gris están las simulaciones y en rojo los datos observados de las remesas:


```{r, echo = FALSE}
datos_sim <- map(1:70, ~ simular_lognormal(.x, llegadas_tbl_2, reg_2)) 
gg_obs <- ggsurvplot(survfit(Surv(tiempo_huso, status)~ state_abbr, llegadas_tbl_2))
datos_obs <- gg_obs$data.survplot %>% mutate(id = 71)
datos_check <- bind_rows(datos_obs, datos_sim)
ggplot(datos_check %>% filter(id!=71), aes(x = time, y = surv, group = id)) +
  geom_step( alpha = 0.3, colour = "gray") +
  geom_step(data = datos_check %>% filter(id==71), colour = "red") +
  facet_wrap(~ strata)
```

- **Checar Nayarit** que presenta un desajuste considerable. Quizá usar distinto
modelo paramétrico por estado.

## Evaluación de tamaño de muestra y hora de salida

Ahora simulamos muestras de distintos tamaños, con horas de llegada. Hacemos cortes en distintos tiempos

- Muestras más grandes acumulan más muestra antes, lo cual reduce su varianza
- Evaluamos en cada hora de salida el sesgo, varianza y error cuadrático medio de 
los estimadores puntuales.

En este ejemplo consideraremos la proporción de voto estimado para RAC en 
Michoacán

```{r}
# cortes (horas después de 18:30 en la elección de 2018)
cortes <- c(1.5, 2.5, 3, 3.5, 4, 4.5, 5.5)
# tamaño de muestra
props_muestra <- c(0.05, 0.10, 0.20)
estado_sim <- "MICH"
n_sim <- 400
```


```{r, echo = FALSE}
simular_cortes <- function(cortes = cortes, prop_muestra = 0.3) {
  evaluacion_tbl <- map(cortes, function(corte) {  
    reps_tbl <- map(1:n_sim, function(rep){
      # estado y porcentaje
      muestra_tbl <- seleccionar_muestra(conteo, prop = prop_muestra, estado_sim)
      tiempos_sim <- simular_lognormal(1, muestra_tbl, reg_2, solo_tiempos = TRUE)
      datos <- bind_cols(tiempos_sim, muestra_tbl %>% select(-state_abbr)) %>% 
        arrange(tiempo) %>% 
        mutate(acumulado_cand = cumsum(RAC_1), 
               acumulado_tot = cumsum(TOTAL_VOTOS_CALCULADOS),
               num_casillas = row_number()) %>% 
        mutate(prop_cand = acumulado_cand / acumulado_tot)
    #prop_corte <- datos %>% filter(num_casillas <= corte) %>% pull(prop_amlo) %>% last
    #hora_salida <- datos %>% filter(num_casillas <= corte) %>% pull(tiempo) %>% last
    prop_corte <- datos %>% filter(tiempo <= corte) %>% pull(prop_cand) %>% last
    hora_salida <- datos %>% filter(tiempo <= corte) %>% pull(tiempo) %>% last
    prop <- datos %>% pull(prop_cand) %>% last
    tibble(prop_corte = prop_corte, prop = prop, hora_salida = hora_salida)
  }) %>% bind_rows %>% 
      mutate(corte = corte)
  }) %>% bind_rows %>% 
    mutate(prop_muestra = prop_muestra)
}
```


```{r, echo = FALSE}
eval_tbl_1 <- simular_cortes(cortes, props_muestra[1]) 
eval_tbl_2 <- simular_cortes(cortes, props_muestra[2]) 
eval_tbl_3 <- simular_cortes(cortes, props_muestra[3]) 
evals_tbl <- bind_rows(eval_tbl_1, eval_tbl_2, eval_tbl_3)
```


En primer lugar, **el error cuadrático medio es menor si se selecciona una muestra
inicial más grande**, y se reduce conforme la hora de censura es más tarde:


```{r, echo = FALSE}
total <- seleccionar_muestra(conteo, prop = 1, "MICH")
prop_real <- sum(total$RAC_1, na.rm = T)/
  sum(total$TOTAL_VOTOS_CALCULADOS, na.rm = T)
```

```{r, echo = FALSE}
ecm_tbl <- evals_tbl %>% mutate(prop_v = prop_real) %>% 
  group_by(corte, prop_muestra) %>% 
  summarise(sesgo_2 = (mean(prop_corte) - prop_real)^2,
            varianza = var(prop_corte), .groups = "drop") %>% 
  ungroup %>% 
  mutate(recm = sqrt(sesgo_2 + varianza)) %>% 
  mutate(prop_muestra = factor(prop_muestra))
#ecm_tbl
ggplot(ecm_tbl, aes(x = corte, y = recm, 
                    colour = prop_muestra, 
                    group=prop_muestra)) +
  geom_point() + geom_line() + geom_vline(xintercept = 4) +
  ylab("Raíz de Error Cuadrático Medio") + 
  xlab("Hora de censura (horas después de 18:30)")
```

Sin embargo, la **proporción del error que se debe a sesgo es más grande
cuanto más grande sea la muestra inicial**:

```{r, echo = FALSE}
ggplot(ecm_tbl, aes(x = corte, y = sesgo_2 / recm^2, 
                    colour = prop_muestra, 
                    group=prop_muestra)) +
geom_point() + geom_line() + geom_vline(xintercept = 4) +
  ylab(expression(Sesgo^2 / ECM)) + 
  xlab("Hora de censura (horas después de 18:30)")
```






